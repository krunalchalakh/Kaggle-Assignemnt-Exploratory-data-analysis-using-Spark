{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.SparkSession, The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and \n",
    "# read parquet files.To create a SparkSession, use the following builder pattern:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark NYC Parking Tickets\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = spark.read.format(\"csv\").load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFame will have columns, and we use a schema to define them.\n",
    "df.printSchema()\n",
    "\n",
    "# printSchema returns schema in tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,StringType,true),StructField(_c5,StringType,true),StructField(_c6,StringType,true),StructField(_c7,StringType,true),StructField(_c8,StringType,true),StructField(_c9,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema\n",
    "#Returns the schema of this DataFrame as a pyspark.sql.types.StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"Summons_Number\", StringType(), True),\n",
    "  StructField(\"Plate_ID\", StringType(), True),\n",
    "  StructField(\"Registration_State\", StringType(), True),\n",
    "  StructField(\"Issue_Date\", StringType(), True),\n",
    "  StructField(\"Violation_Code\", StringType(), True),\n",
    "  StructField(\"Vehicle_Body_Type\", StringType(), True),\n",
    "  StructField(\"Vehicle_Make\", StringType(), True),\n",
    "  StructField(\"Violation_Precinct\", StringType(), True),\n",
    "  StructField(\"Issuer_Precinct\", StringType(), True),\n",
    "  StructField(\"Violation_Time\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(myManualSchema).load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: string (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: string (nullable = true)\n",
      " |-- Violation_Code: string (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: string (nullable = true)\n",
      " |-- Issuer_Precinct: string (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Sample Data\n",
    "#########################\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "# Assumption : Lots of Data Columns are having NULL but dont want to drop them since it is just EDA aand not Model Building\n",
    "##############################################################################################################################\n",
    "\n",
    "# Above equivalent sql syntax\n",
    "# register DataFrame as temp table\n",
    "df.createOrReplaceTempView(\"NYC_Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(distinct_ticket_count=10803029)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "# EDA 1 : 1.Find the total number of tickets for the year.\n",
    "##########################################################################\n",
    "# Summons_Number is taken as Ticket Number here\n",
    "# Assumption that ALl Data is 2017 Only and no Year Condition is added.\n",
    "\n",
    "DF_sql = spark.sql(\"SELECT count(distinct Summons_Number) as distinct_ticket_count FROM NYC_Table\")\n",
    "\n",
    "DF_sql.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# 2.Find out the number of unique states from where the cars that got parking tickets came. \n",
    "# (Hint: Use the column 'Registration State'.)\n",
    "# There is a numeric entry '99' in the column, which should be corrected.\n",
    "# Replace it with the state having the maximum entries. Provide the number of unique states again.\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ticket_count_state=8481061, Registration_State='NY'),\n",
       " Row(ticket_count_state=925965, Registration_State='NJ'),\n",
       " Row(ticket_count_state=285419, Registration_State='PA'),\n",
       " Row(ticket_count_state=144556, Registration_State='FL'),\n",
       " Row(ticket_count_state=141088, Registration_State='CT')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_sql = spark.sql(\"SELECT count(distinct Summons_Number) as ticket_count_state,Registration_State FROM NYC_Table Group By Registration_State Order By ticket_count_state DESC\")\n",
    "\n",
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# NY is the State with Maximum Parking Tickets as is the Case Study on New York Only :-)\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark2.4/python/pyspark/sql/dataframe.py:1793: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
      "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# Replace 99 with NY in the Column Registration_State\n",
    "#########################################################################\n",
    "replace = {'99':'NY'}\n",
    "df = df.na.replace(replace,1,\"Registration_State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ticket_count_state=8517686, Registration_State='NY'),\n",
       " Row(ticket_count_state=925965, Registration_State='NJ'),\n",
       " Row(ticket_count_state=285419, Registration_State='PA'),\n",
       " Row(ticket_count_state=144556, Registration_State='FL'),\n",
       " Row(ticket_count_state=141088, Registration_State='CT')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the State with Maximum Entries of Summons.\n",
    "\n",
    "# Above equivalent sql syntax\n",
    "# register DataFrame as temp table\n",
    "df.createOrReplaceTempView(\"NYC_Table\")\n",
    "\n",
    "DF_sql = spark.sql(\"SELECT count(distinct Summons_Number) as ticket_count_state,Registration_State FROM NYC_Table Group By Registration_State Order By ticket_count_state DESC\")\n",
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "# Value of the NY is Increased from 8481061 To 8517686 and hence replacement is done\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(unique_state_count=67)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################\n",
    "# EDA 2 : NO of States with Parking Tickets\n",
    "################################################################################################\n",
    "DF_sql = spark.sql(\"SELECT count(distinct Registration_State) as unique_state_count FROM NYC_Table\")\n",
    "DF_sql.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# EDA 2 : No of Unique Registration States with Parking Tickets - 67\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Aggregation Tasks\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(violation_code_count=1528588, Violation_Code='21'),\n",
       " Row(violation_code_count=1400614, Violation_Code='36'),\n",
       " Row(violation_code_count=1062304, Violation_Code='38'),\n",
       " Row(violation_code_count=893498, Violation_Code='14'),\n",
       " Row(violation_code_count=618593, Violation_Code='20')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. How often does each violation code occur? Display the frequency of the top five violation codes.\n",
    "\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as violation_code_count,Violation_Code FROM NYC_Table Group By Violation_Code ORDER BY violation_code_count DESC\")\n",
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# Top 5 Violation Code are 21,36,38,14,20\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.How often does each 'vehicle body type' get a parking ticket? \n",
    "## How about the 'vehicle make'? \n",
    "## (Hint: Find the top 5 for both.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(violation_summon_count=3719802, Vehicle_Body_Type='SUBN'),\n",
       " Row(violation_summon_count=3082020, Vehicle_Body_Type='4DSD'),\n",
       " Row(violation_summon_count=1411970, Vehicle_Body_Type='VAN'),\n",
       " Row(violation_summon_count=687330, Vehicle_Body_Type='DELV'),\n",
       " Row(violation_summon_count=438191, Vehicle_Body_Type='SDN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########\n",
    "DF_sql = spark.sql(\"SELECT Count(Summons_Number) as violation_summon_count,Vehicle_Body_Type FROM NYC_Table Group By Vehicle_Body_Type ORDER BY violation_summon_count DESC\")\n",
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# Top 5 Violation Body type are SUBN / 4DSD / VAN / DELV and SDN\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(violation_summon_count=1280958, Vehicle_Make='FORD'),\n",
       " Row(violation_summon_count=1211451, Vehicle_Make='TOYOT'),\n",
       " Row(violation_summon_count=1079238, Vehicle_Make='HONDA'),\n",
       " Row(violation_summon_count=918590, Vehicle_Make='NISSA'),\n",
       " Row(violation_summon_count=714655, Vehicle_Make='CHEVR')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########\n",
    "DF_sql = spark.sql(\"SELECT Count(Summons_Number) as violation_summon_count,Vehicle_Make FROM NYC_Table Group By Vehicle_Make ORDER BY violation_summon_count DESC\")\n",
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# Top 5 Vehicle Body Type are FORD / TOYOTA / HONDA / NISSAN and Chevrolet\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "## 3.A precinct is a police station that has a certain zone of the city under its command. \n",
    "## Find the (5 highest) frequencies of tickets for each of the following:\n",
    "## 1.'Violation Precinct' (This is the precinct of the zone where the violation occurred). \n",
    "## Using this, can you draw any insights for parking violations in any specific areas of the city?\n",
    "## 2.\t'Issuer Precinct' (This is the precinct that issued the ticket.)\n",
    "## Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. \n",
    "## These are erroneous entries. Hence, you need to provide the records for five correct precincts. \n",
    "# (Hint: Print the top six entries after sorting.)\n",
    "###########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(violation_summon_count=2072400, Violation_Precinct='0'),\n",
       " Row(violation_summon_count=535671, Violation_Precinct='19'),\n",
       " Row(violation_summon_count=352450, Violation_Precinct='14'),\n",
       " Row(violation_summon_count=331810, Violation_Precinct='1'),\n",
       " Row(violation_summon_count=306920, Violation_Precinct='18'),\n",
       " Row(violation_summon_count=296514, Violation_Precinct='114')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Violation Precinct\n",
    "DF_sql = spark.sql(\"SELECT Count(Summons_Number) as violation_summon_count,Violation_Precinct FROM NYC_Table Group By Violation_Precinct ORDER BY violation_summon_count DESC\")\n",
    "DF_sql.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## \n",
    "## the'Violating Precinct' as '0'are erroneous entries.\n",
    "## hence top five 'Violating Precinct' are 19,14,1,18 and 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(violation_summon_count=2388479, Issuer_Precinct='0'),\n",
       " Row(violation_summon_count=521513, Issuer_Precinct='19'),\n",
       " Row(violation_summon_count=344977, Issuer_Precinct='14'),\n",
       " Row(violation_summon_count=321170, Issuer_Precinct='1'),\n",
       " Row(violation_summon_count=296553, Issuer_Precinct='18'),\n",
       " Row(violation_summon_count=289950, Issuer_Precinct='114')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Issuer Precinct\n",
    "DF_sql = spark.sql(\"SELECT Count(Summons_Number) as violation_summon_count,Issuer_Precinct FROM NYC_Table Group By Issuer_Precinct ORDER BY violation_summon_count DESC\")\n",
    "DF_sql.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## \n",
    "## the 'Issuing Precinct' as '0'are erroneous entries.\n",
    "## hence top five Issuing Precinct' are 19,14,1,18 and 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 4.Find the violation code frequencies for three precincts that have issued the most number of tickets. \n",
    "## Do these precinct zones have an exceptionally high frequency of certain violation codes? \n",
    "## Are these codes common across precincts? \n",
    "## (Hint: In the SQL view, use the 'where' attribute to filter among three precincts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_count=2072400, Violation_Precinct='0'),\n",
       " Row(Violation_Code_count=535671, Violation_Precinct='19'),\n",
       " Row(Violation_Code_count=352450, Violation_Precinct='14'),\n",
       " Row(Violation_Code_count=331810, Violation_Precinct='1')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## violation code frequencies for three precincts that have issued the most number of tickets i.e. for 19,14 and 1\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as Violation_Code_count,Violation_Precinct FROM NYC_Table Group By Violation_Precinct ORDER BY Violation_Code_count DESC\")\n",
    "DF_sql.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_count=1528588, Violation_Code='21'),\n",
       " Row(Violation_Code_count=1400614, Violation_Code='36'),\n",
       " Row(Violation_Code_count=1062304, Violation_Code='38')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## top 3 violation codes frequency\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as Violation_Code_count,Violation_Code FROM NYC_Table Group By Violation_Code ORDER BY Violation_Code_count DESC\")\n",
    "DF_sql.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Yes these precinct zones have an exceptionally high frequency of certain violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_count=90530, Violation_Code='46'),\n",
       " Row(Violation_Code_count=74926, Violation_Code='38'),\n",
       " Row(Violation_Code_count=73359, Violation_Code='37')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## top 3 violation codes for Violation_Precinct='19'\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as Violation_Code_count,Violation_Code FROM NYC_Table where Violation_Precinct = '19' Group By Violation_Code ORDER BY Violation_Code_count DESC \")\n",
    "DF_sql.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_count=75850, Violation_Code='14'),\n",
       " Row(Violation_Code_count=58032, Violation_Code='69'),\n",
       " Row(Violation_Code_count=40150, Violation_Code='31')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## top 3 violation codes for Violation_Precinct='14'\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as Violation_Code_count,Violation_Code FROM NYC_Table where Violation_Precinct = '14' Group By Violation_Code ORDER BY Violation_Code_count DESC \")\n",
    "DF_sql.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_count=76375, Violation_Code='14'),\n",
       " Row(Violation_Code_count=39197, Violation_Code='16'),\n",
       " Row(Violation_Code_count=28768, Violation_Code='20')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## top 3 violation codes for Violation_Precinct='1'\n",
    "DF_sql = spark.sql(\"SELECT Count(Violation_Code) as Violation_Code_count,Violation_Code FROM NYC_Table where Violation_Precinct = '1' Group By Violation_Code ORDER BY Violation_Code_count DESC \")\n",
    "DF_sql.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### top three codes are not common across precincts 19,14 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  5. Find out the properties of parking violations across different times of the day:\n",
    "##Find a way to deal with missing values, if any.\n",
    "#(Hint: Check for the null values using 'isNull' under the SQL. \n",
    " #Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    "    \n",
    "##The Violation Time field is specified in a strange format. \n",
    "#Find a way to make this a time attribute that you can use to divide into groups.\n",
    "\n",
    "\n",
    "##Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. \n",
    "#For each of these groups, find the three most commonly occurring violations.\n",
    "#(Hint: Use the CASE-WHEN in SQL view to segregate into bins. \n",
    "#To find the most commonly occurring violations, you can use an approach similar to the one mentioned in hint for question 4.)\n",
    "\n",
    "\n",
    "##Now, try another direction. For the three most commonly occurring violation codes, \n",
    "#find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_sql = spark.sql(\"SELECT * FROM NYC_Table WHERE ISNULL(Violation_Time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as we getting 0 results we are go to go  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 7.The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department.\n",
    "# Let’s take an example of estimating this for the three most commonly occurring codes:\n",
    "\n",
    "## Find the total occurrences of the three most common violation codes.\n",
    "\n",
    "## Then, visit the website:\n",
    "# http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "\n",
    "    ##It lists the fines associated with different violation codes. They’re divided into two categories: \n",
    "# one for the highest-density locations in the city and the other for the rest of the city. \n",
    "# For the sake of simplicity, take the average of the two.\n",
    "# Using this information, find the total amount collected for the three violation codes with the maximum tickets.\n",
    "# State the code that has the highest total collection.\n",
    "# What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_Sum=3991506)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the total occurrences of the three most common violation codes.\n",
    "# as we alredy discovered that 21,36 and 38 are three most common violation codes\n",
    "\n",
    "DF_sql = spark.sql(\"SELECT count(Violation_Code) as Violation_Code_Sum FROM NYC_Table where Violation_Code='21' OR Violation_Code='36'OR Violation_Code='38'\")\n",
    "DF_sql.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_amount=84072340.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## amount collected for the violation code 21\n",
    "\n",
    "DF_sql1 = spark.sql(\"SELECT (Count(Violation_Code)*(45+65)/2) as Violation_Code_amount FROM NYC_Table where Violation_Code='21' group by Violation_Code \")\n",
    "DF_sql1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_amount=70030700.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## amount collected for the violation code 36\n",
    "\n",
    "DF_sql2 = spark.sql(\"SELECT (Count(Violation_Code)*(50+50)/2) as Violation_Code_amount FROM NYC_Table where Violation_Code='36' group by Violation_Code \")\n",
    "DF_sql2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Violation_Code_amount=53115200.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## amount collected for the violation code 38\n",
    "\n",
    "DF_sql3 = spark.sql(\"SELECT (Count(Violation_Code)*(65+35)/2) as Violation_Code_amount FROM NYC_Table where Violation_Code='38' group by Violation_Code \")\n",
    "DF_sql3.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Violation_Code 21 has highest collection \n",
    "## No parking (where parking is not allowed by sign, street marking or traffic control device) is most ignored violation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
